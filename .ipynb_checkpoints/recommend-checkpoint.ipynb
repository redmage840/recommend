{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTES\n",
    "\n",
    "# compare the ordered gameid vecs by iterating over test vec and subtracting from a val for the \n",
    "# distance, by index, when compared to pred vec\n",
    "\n",
    "# 'seven' (mean/mode) guess against test data is more accurate than predictions, according\n",
    "# to rmse, but is useless for actual rec system (same rating for all userid-gameid pairs)...\n",
    "\n",
    "# instead, design test for confirming recognition of highest rated pairs among test data...\n",
    "# test this by ordering all userID-gameID predictions (desc) and comparing to order from test data\n",
    "# how to compare similarity of lists of unique gameIDs? ...\n",
    "# also the test vector of gameIDs will not have ncsrly been rated although they do have a pred rating...\n",
    "\n",
    "# k nearest villains - recs based on most dissimilar users' lowest rated?...\n",
    "\n",
    "# possible check for outliers (dist of ratings vastly different vectors, all one value)\n",
    "\n",
    "# Consider cross-validation for more accurate testing\n",
    "# sklearn cross-validation module is the equiv of just shuffling train/test split, retraining, rerun\n",
    "\n",
    "# Consider wrapping in custom sklearn estimator / classifier, maybe inherit from base classes\n",
    "\n",
    "# README\n",
    "# Datafield has 'user_ids' and their 'rating' (1-10) of various 'game_ids'\n",
    "# Fields = userID , gameID , rating\n",
    "# Prediction of unrated games for each user_id is made using:\n",
    "# k-nearest neighbors found using cosine-similarity\n",
    "# predicted rating based on those near-neighbors' rating weighted by cosine-similarity users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('inputs/boardgame-elite-users.csv')\n",
    "\n",
    "train, test = train_test_split(df, test_size=.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ptable = train.pivot_table(index='userID', columns='gameID', values='rating') \n",
    "\n",
    "test_ptable = test.pivot_table(index='userID', columns='gameID', values='rating') \n",
    "\n",
    "# get mean before fillna\n",
    "# this is mean of rating for gameids using train data\n",
    "# test by applying to every userid-gameid pair in test corresponding to gameid\n",
    "i = 0\n",
    "gameID_mean_tuples = []\n",
    "for row in train_ptable.mean():\n",
    "    gameID_mean_tuples.append((train_ptable.columns[i], row))\n",
    "    i += 1\n",
    "\n",
    "train_ptable = train_ptable.fillna(0)\n",
    "test_ptable = test_ptable.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "norm_train = normalize(train_ptable, copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "sim = cosine_similarity(norm_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbor_indx = sim.argsort()[:,::-1]\n",
    "\n",
    "# HELPER FUNCTIONS\n",
    "def get_users_that_rated(gameid):\n",
    "    return train_ptable[gameid].where(train_ptable[gameid]>0).dropna().index.tolist()\n",
    "    \n",
    "def get_k_nearest_neighbors(userid, gameid, k):\n",
    "    assert(k>1)\n",
    "    pot_nbors = get_users_that_rated(gameid)\n",
    "    userid_indx = train_ptable.index.get_loc(userid)\n",
    "    \n",
    "    sim_userIDs = []\n",
    "    for x in nbor_indx[userid_indx]:\n",
    "        if train_ptable.index[x] in pot_nbors:\n",
    "            sim_userIDs.append(train_ptable.index[x])\n",
    "        if len(sim_userIDs)>k:\n",
    "            break\n",
    "    return sim_userIDs[1:]\n",
    "\n",
    "def round_to_point5(someFloat):\n",
    "    score = np.around(someFloat, decimals=1)\n",
    "    first_digit = score // 1\n",
    "    last_digit = (score % 1) * 10\n",
    "    carry = 0\n",
    "    if 0 <= last_digit <= 2:\n",
    "        return float(first_digit)\n",
    "    elif 2 < last_digit <= 7:\n",
    "        return float(first_digit + .5)\n",
    "    elif last_digit > 7:\n",
    "        return float(first_digit + 1)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Takes userID, gameID, optional k\n",
    "# Returns predicted rating for gameID\n",
    "def predict_rating(uid, gameid, k=10):\n",
    "    sim_uids = get_k_nearest_neighbors(uid, gameid, k)\n",
    "    accm_score = 0.0\n",
    "    accm_weight = 0.0\n",
    "    for nbor_id in sim_uids:\n",
    "        sim_indx_x = train_ptable.index.get_loc(uid)\n",
    "        sim_indx_y = train_ptable.index.get_loc(nbor_id)\n",
    "        accm_score += (train_ptable.loc[nbor_id][gameid] * sim[sim_indx_x][sim_indx_y])\n",
    "        accm_weight += sim[sim_indx_x][sim_indx_y]\n",
    "    score = accm_score / accm_weight\n",
    "    score = round_to_point5(score)\n",
    "    return score\n",
    "\n",
    "predict_rating(5480, 97903)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is iteration slowing down? try apply or something\n",
    "\n",
    "user_ids = []\n",
    "\n",
    "for row in test_ptable.itertuples():\n",
    "    user_ids.append(row[0])\n",
    "    \n",
    "game_ids = []\n",
    "for x in test_ptable.columns:\n",
    "    game_ids.append(x)\n",
    "\n",
    "# tmpdict = {'user_ID':}...\n",
    "\n",
    "pred_matrix = []\n",
    "for user in user_ids:\n",
    "    row = []\n",
    "    for gid in game_ids:\n",
    "        row.append(predict_rating(user,gid))\n",
    "    pred_matrix.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create user_game_matrix, which holds list predicted gameIDs for each userID, desc order\n",
    "\n",
    "user_game_matrix = []\n",
    "for usr_row in pred_matrix:\n",
    "    sorted_row_indexes = np.asarray(usr_row).argsort()[::-1]\n",
    "    \n",
    "    \n",
    "    gids = []\n",
    "    for indx in sorted_row_indexes:\n",
    "        gids.append(train_ptable.columns[indx])\n",
    "    user_game_matrix.append(gids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate matrix of actual ratings from user_id/game_id pairs\n",
    "truth_matrix = []\n",
    "for user in user_ids:\n",
    "    row = []\n",
    "    for gid in game_ids:\n",
    "        row.append(test_ptable.loc[user][gid])\n",
    "    truth_matrix.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get gameIDs for each userID from truth_matrix\n",
    "\n",
    "truth_game_matrix = []\n",
    "for usr_row in truth_matrix:\n",
    "    sorted_row_indexes = np.asarray(usr_row).argsort()[::-1]\n",
    "    \n",
    "    gids = []\n",
    "    for indx in sorted_row_indexes:\n",
    "        gids.append(test_ptable.columns[indx])\n",
    "    truth_game_matrix.append(gids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of tuples with every rating from userID-gameID pairs in test data as first elem,\n",
    "# predicted rating as second elem of tuples\n",
    "compare_tuples = []\n",
    "\n",
    "for i, row in enumerate(truth_matrix):\n",
    "    for j, rating in enumerate(row):\n",
    "        if rating > .1:\n",
    "            compare_tuples.append((truth_matrix[i][j], pred_matrix[i][j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse =  1.5437435898711558\n"
     ]
    }
   ],
   "source": [
    "# find rmse in all compare_tuples\n",
    "\n",
    "import math\n",
    "\n",
    "mean_sqr_sum = 0\n",
    "i = 0\n",
    "\n",
    "for pair in compare_tuples:\n",
    "    mean_sqr_sum += ((pair[0] - pair[1])**2)\n",
    "    i += 1\n",
    "\n",
    "print('rmse = ', math.sqrt(mean_sqr_sum/i))\n",
    "# this is a measure of acc of all pred\n",
    "# check this measure of acc by comparing to a matrix of random guesses, mean guess for all (7)\n",
    "\n",
    "# need to just get top N pred for each user\n",
    "# then see if those suggestions are among highest rated in test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse =  1.6411267101123053\n"
     ]
    }
   ],
   "source": [
    "# rmse of all sevens\n",
    "# seven is both the mode and median (i think, test/show this)\n",
    "\n",
    "mean_sqr_sum = 0\n",
    "i = 0\n",
    "\n",
    "for pair in compare_tuples:\n",
    "    mean_sqr_sum += ((pair[0] - 7)**2)\n",
    "    i += 1\n",
    "\n",
    "print('rmse = ', math.sqrt(mean_sqr_sum/i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sort-distance of predicted user-game precedence list when compared to test data prec list (each user)\n",
    "# compare with cross-val/reshuffle train-test, using different portions of prec list\n",
    "# try Kendall-Tau distance metric\n",
    "# KT in brief, to compare lists of uniq gameIDs for each user,\n",
    "# for each pair of gameIDs, add one to KT total if that pair is in opposite order in the 2 lists\n",
    "# do this for each user's list of uniq gameIDs (preference list)\n",
    "# normalize with /n(n-1)2, where n is list len\n",
    "\n",
    "# first get pred and truth list of gameIDs\n",
    "# this could be a df with rows and columns of gameIDs\n",
    "# each cell of this row would be filled with zero or one,\n",
    "# based on if ((index-gameX-predlist < index-gameY-predlist) == (index-gameX-truthlist < index-gameY-truthlist))\n",
    "# need to account for when the indices are exactly equal in the above"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
